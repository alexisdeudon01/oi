
# IDS2 ‚Äî SOC IDS sur Raspberry Pi 5

Ce projet impl√©mente un **pipeline SOC IDS complet**, robuste et automatis√©, bas√© sur **Suricata**, **Vector**, **Redis** et **AWS OpenSearch**, d√©ploy√© sur un **Raspberry Pi 5 (8 GB RAM)**.
L‚Äôarchitecture est con√ßue pour fonctionner **24/7**, sous contraintes de ressources, avec **parall√©lisme contr√¥l√©**, **backpressure**, **observabilit√© compl√®te** et **pilotage local via interface Web**.

---

## Table des mati√®res

1. Objectifs du projet
2. P√©rim√®tre et principes g√©n√©raux
3. Plateforme mat√©rielle et contraintes
4. Vue d‚Äôensemble de l‚Äôarchitecture
5. Flux de donn√©es (pipeline SOC)
6. R√¥le des composants
7. Organisation des services (systemd & Docker)
8. Gestion des ressources (CPU / RAM / disque)
9. Gestion des logs et de la m√©moire
10. Parall√©lisme et multi-process
11. S√©curit√© r√©seau
12. Observabilit√© et pilotage Web
13. Automatisation et d√©ploiement
14. Exploitation et cycle de vie
15. R√©sum√© final
16. **[Documentation UML / Architecture](docs/uml/README.md)** üé®

---

## 1. Objectifs du projet

Les objectifs principaux sont :

* D√©ployer un **IDS passif** bas√© sur Suricata via **port mirroring**
* Centraliser les √©v√©nements de s√©curit√© dans **AWS OpenSearch**
* Garantir la **stabilit√© du syst√®me** sous forte charge
* Ne **jamais d√©passer 70 % de CPU et de RAM**
* Fournir une **observabilit√© compl√®te** (logs, m√©triques, dashboards)
* Permettre un **pilotage local sans SSH** (interface Web)
* Automatiser **100 % du d√©ploiement** apr√®s un reset usine du Pi

---

## 2. P√©rim√®tre et principes g√©n√©raux

### Ce que fait le projet

* Capture passive du trafic r√©seau
* D√©tection IDS (alertes, anomalies)
* Transformation des √©v√©nements en format ECS
* Bufferisation intelligente en cas de surcharge
* Ingestion s√©curis√©e vers OpenSearch
* Supervision continue des ressources
* Administration locale centralis√©e

### Ce que le projet ne fait pas

* Pas d‚ÄôIPS (aucun blocage r√©seau)
* Pas d‚Äôinspection de paquets en Python
* Pas de stockage long terme local
* Pas d‚Äôexposition directe √† Internet

---

## 3. Plateforme mat√©rielle et contraintes

### Raspberry Pi cible

| √âl√©ment          | Valeur                       |
| ---------------- | ---------------------------- |
| Mod√®le           | Raspberry Pi 5               |
| CPU              | 4 √ó Cortex-A76               |
| RAM              | 8 GB                         |
| OS               | Debian GNU/Linux 13 (Trixie) |
| IP               | **192.168.178.66**           |
| Interface r√©seau | **eth0 uniquement**          |
| Swap             | 2 GB                         |
| Stockage         | microSD 119 GB               |

### Contraintes strictes

* CPU total ‚â§ **70 %**
* RAM totale ‚â§ **70 %**
* Fonctionnement continu (24/7)
* R√©sistance aux pics de trafic (burst IDS)
* Aucun appel bloquant dans la boucle critique

---

## 4. Vue d‚Äôensemble de l‚Äôarchitecture

### Architecture logique

```
Trafic r√©seau (mirroring)
        ‚Üì
     Suricata
        ‚Üì
    eve.json (RAM)
        ‚Üì
      Vector
        ‚Üì
      Redis (buffer)
        ‚Üì
 AWS OpenSearch
        ‚Üì
 Dashboards & Alertes
```

### Architecture physique

* **Raspberry Pi** : capture, transformation, orchestration
* **AWS** : indexation, recherche, visualisation distante

---

## 5. Flux de donn√©es (pipeline SOC)

1. Le trafic r√©seau est dupliqu√© via **port mirroring**
2. Suricata capture les paquets sur `eth0`
3. Les √©v√©nements sont √©crits dans `eve.json`
4. Vector lit les √©v√©nements en temps r√©el
5. Les √©v√©nements sont mapp√©s au format **ECS**
6. Redis absorbe les pics si OpenSearch ralentit
7. Les donn√©es sont envoy√©es en **bulk HTTPS**
8. Les dashboards affichent les alertes et m√©triques

---

## 6. R√¥le des composants

### Suricata

* IDS haute performance (C / kernel)
* Capture passive uniquement
* Fonctionne **hors Docker**
* √âcrit exclusivement en local
* G√©r√© comme un service `systemd`

### Vector

* Collecte et transformation des logs
* Mapping ECS natif
* Batching, retry, backoff
* Fonctionne en **Docker**

### Redis

* Buffer de s√©curit√©
* Backpressure
* √âvite la perte de logs

### AWS OpenSearch

* Indexation et recherche
* Stockage long terme
* Dashboards et alertes

### Agent SOC Python

* Orchestrateur central
* Multi-process
* Supervision CPU/RAM
* Pilotage systemd & Docker
* Backend de la Web UI

---

## 7. Organisation des services

### systemd (host)

* `network-eth0-only.service` : force `eth0` uniquement
* `suricata.service` : capture IDS
* `ids2-agent.service` : orchestration SOC

### Docker

* Vector
* Redis
* Prometheus
* Grafana
* FastAPI (control plane)
* cAdvisor
* Node Exporter

Chaque service est **isol√©**, supervis√© et red√©marr√© automatiquement.

---

## 8. Gestion des ressources (CPU / RAM)

### R√©partition CPU

* Suricata : ~3 c≈ìurs
* Vector : ~1 c≈ìur
* Redis : ~0.5 c≈ìur
* Prometheus : ~0.2 c≈ìur
* Grafana : ~0.2 c≈ìur
* FastAPI : ~0.5 c≈ìur
* cAdvisor : ~0.1 c≈ìur
* Node Exporter : ~0.1 c≈ìur

### R√©partition RAM

* Suricata : ~4 GB
* Vector : ~1 GB
* Redis : ~512 MB
* Prometheus : ~256 MB
* Grafana : ~256 MB
* FastAPI : ~256 MB
* cAdvisor : ~64 MB
* Node Exporter : ~64 MB
* Libre : >1 GB

### M√©canismes de contr√¥le

* Limites systemd (`CPUQuota`, `MemoryMax`)
* Limites Docker (`cpus`, `mem_limit`)
* Throttling dynamique par l‚Äôagent
* Backpressure Redis
* Batching Vector

---

## 9. Gestion des logs et de la m√©moire

### Logs Suricata

* Stock√©s en **RAM disk**
* Taille maximale strictement born√©e
* Rotation agressive
* Aucun historique local conserv√©

### M√©moire

* Aucun cache applicatif long terme
* Garbage collection Python forc√©e
* Nettoyage p√©riodique du cache kernel
* Swappiness faible

Objectif : **z√©ro fuite m√©moire**, m√™me apr√®s plusieurs semaines.

---

## 10. Parall√©lisme et multi-process

### Agent SOC

L‚Äôagent est d√©coup√© en **processus ind√©pendants** :

* Superviseur
* Contr√¥le ressources
* Tests r√©seau (async)
* Monitoring / m√©triques
* V√©rification ingestion (optionnel)

### B√©n√©fices

* Isolation m√©moire
* R√©silience
* Exploitation optimale des 4 c≈ìurs
* Pas de contention GIL critique

---

## 11. S√©curit√© r√©seau

* Une seule interface active : **eth0**
* Mode promiscuous activ√©
* Firewall sortant strict (HTTPS + DNS)
* Aucune exposition Internet
* Administration uniquement LAN

---

## 12. Observabilit√© et pilotage Web

### Observabilit√©

* Prometheus : m√©triques
* Grafana : dashboards SOC
* OpenSearch : analyse s√©curit√©

### Web Control Plane

* Endpoint HTTP `/status` pour le statut du pipeline
* Endpoint `/health` pour health check
* Visualisation √©tat pipeline (composants, m√©triques)
* CPU / RAM / d√©bit
* Modification des param√®tres (via API)
* Red√©marrage des services (via API)
* Gestion Docker

Aucun acc√®s SSH requis pour l'exploitation courante.

---

## 13. Automatisation et d√©ploiement

* Le script `deploy/push_to_pi.sh` ou `python -m ids.deploy.pi_uploader` permet le d√©ploiement complet
* V√©rifie la connectivit√© (SSH, Docker, AWS)
* Build et push de l'image Docker vers le Pi
* Synchronise les fichiers n√©cessaires (config, secrets, code)
* Les credentials AWS sont fournis via `secret.json` (copier `secret.json.example`)
* G√©n√®re `docker/.env` pour injecter AWS/endpoint dans Docker Compose
* Active les services systemd et Docker Compose
* Idempotent et rejouable
* Fonctionne apr√®s reset usine du Pi
* Configure r√©seau, services, Docker, agent
* D√©marrage automatique au boot

---

## 14. Exploitation et cycle de vie

* D√©marrage automatique
* Supervision continue
* Red√©marrage en cas de crash
* Mise √† jour contr√¥l√©e
* Reset complet possible
* Extensible (forensic, IPS, ML‚Ä¶)

---

## 15. R√©sum√© final

‚úî IDS passif haute performance
‚úî Architecture SOC moderne
‚úî Pipeline r√©silient et observable
‚úî Ressources strictement contr√¥l√©es
‚úî Automatisation compl√®te
‚úî Pilotage Web local
‚úî Adapt√© Raspberry Pi 5
‚úî Pr√™t production 24/7

---

## 16. GitHub Actions depuis Codespaces

Les **Codespaces user secrets** ne sont pas disponibles automatiquement dans
GitHub Actions. Pour lancer le workflow CI/CD depuis un Codespace :

1. Copiez `scripts/actions_secrets.map.example` en `scripts/actions_secrets.map`
   et ajustez les noms des variables source.
   * OAuth Tailscale : `TAILSCALE_OAUTH_CLIENT_ID`,
     `TAILSCALE_OAUTH_CLIENT_SECRET`.
   * API Tailscale (check connectivit√©) : `TAILSCALE_TAILNET`,
     `TAILSCALE_API_KEY`.
   * AWS (check connectivit√©) : `AWS_ACCESS_KEY_ID`,
     `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`, `AWS_SESSION_TOKEN` (optionnel).
2. Synchronisez vos secrets vers le d√©p√¥t :
   `scripts/gh_actions_sync_secrets.sh --repo OWNER/REPO`
   * Le script utilise `GITAPI` comme `GH_TOKEN` si pr√©sent.
3. D√©clenchez le workflow :
   `scripts/gh_actions_run.sh --ref main`
   (ou `gh workflow run ci-cd.yml --ref main`).

Pr√©requis : le CLI `gh` doit √™tre install√© et votre token GitHub doit avoir
les droits n√©cessaires (ex: `repo` + `workflow` pour un d√©p√¥t priv√©).
